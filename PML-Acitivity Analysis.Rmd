---
title: "PML - Devices - Personal Activity Analysis"
author: "Srinivasa Irrinki"
date: "August 2, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.

The goal of this project is to predict the manner in which the participants did the exercise. This is the classe variable of the training set, which classifies the correct and incorrect outcomes into A, B, C, D, and E categories. This report describes how the model for the project was built, its cross validation, expected out of sample error calculation, and the choices made. It was used successfully to accurately predict all 20 different test cases.

From the dataset, it is first observed from basic exploratory analysis that only 1/3 of the data is informative. After screening out the uninformative data, the author has tried 4 different machines learning models: random forest, boosting, linear discriminant, and classification trees on subsets of the training data. After a few trials, the random forest model was chosen to generate the answers for the quiz, which achieved 17 correct answers out of 20 questions.

## Data Loading
The first step is to load the training and testing data to R.

```{r}
library(caret)
training = read.csv("C:/Users/admin_ithw/Desktop/Data Science Assignments/Predictive Machine Learning/pml-training.csv")
testing = read.csv("C:/Users/admin_ithw/Desktop/Data Science Assignments/Predictive Machine Learning/pml-testing.csv")
```

## Cleaning the data
```{r}
training1 = training[,-c(1:5,12:36, 50:59, 69:83,87:101,103:112,125:139,141:150)]
testing1 = testing[,-c(1:5,12:36, 50:59, 69:83,87:101,103:112,125:139,141:150)]
set.seed(2233)
folds<-createFolds(y=training1$classe,k=20,list=TRUE,returnTrain=FALSE)
training1_1<-training1[folds$Fold01,]
modFit1_1_rf <- train(classe~., data=training1_1, method = "rf", prox = TRUE)
modFit1_1_gbm<- train(classe~., data=training1_1, method = "gbm",  verbose = FALSE)
modFit1_1_rpart<-train(classe~., data=training1_1, method = "rpart")
modFit1_1_lda<- train(classe~., data=training1_1, method = "lda")

training1_2<-training1[folds$Fold02,]
modFit1_2_rf<- train(classe~., data=training1_2, method = "rf", prox = TRUE)
modFit1_2_gbm<- train(classe~., data=training1_2, method = "gbm",  verbose = FALSE)
```
On the second subset, accuracy of random forest is 88% and that of boosting is 87.2 %. Both still have very high accuracy. We will then deploy the model to test out-of-sample error with a third subset of data
```{r}
training1_3<-training1[folds$Fold03,]
confusionMatrix(training1_3$classe,predict(modFit1_1_rf,training1_3))  
confusionMatrix(training1_3$classe,predict(modFit1_1_gbm,training1_3)) 
training1_4<-training1[folds$Fold04,]
confusionMatrix(training1_4$classe,predict(modFit1_1_rf,training1_4)) 
confusionMatrix(training1_4$classe,predict(modFit1_1_gbm,training1_4)) 

```

From the out-of-sample testing for subset 3 and 4, random forest model still outperforms boosting, with an accuracy rate of over 94%. As such, the random forest model 1 is chosen.
```{r}
predict(modFit1_1_rf,testing1) 
```